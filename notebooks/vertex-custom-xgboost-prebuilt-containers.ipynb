{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b19591-8614-4f15-b180-8ebff497667c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset used for this tutorial is the Iris dataset from TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7851f1f-91ca-48c6-8554-315d6e1c632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = \"--user\"\n",
    "else:\n",
    "    USER_FLAG = \"\"\n",
    "\n",
    "!pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c0502-04e5-42a6-b928-2a8f56103634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -U google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e526864d-f646-4157-b245-ac42677a18e4",
   "metadata": {},
   "source": [
    "## Set up your Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258833e-c8f4-4819-932c-f015389d3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME=f\"gs://{PROJECT_ID}-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dedab9-c111-445e-bccf-a87677a3fbe2",
   "metadata": {},
   "source": [
    "## Set up variables and initialize Vertex SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78484601-ff89-4abc-a9f1-cba76e826efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54aad37-e0e4-4dc9-8038-afabe1f69f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af53e9-8e2e-4066-a650-a33e9b872ef3",
   "metadata": {},
   "source": [
    "## Set machine type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fef42-4cd5-4da8-b168-b38b9130d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d3a3e-05fb-41ea-a967-4bb9375d4c87",
   "metadata": {},
   "source": [
    "## Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e99c2-73c5-476f-9506-a1a5b92b2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM python:3.6.9-buster\n",
    "WORKDIR /root\n",
    "\n",
    "RUN pip install xgboost pandas\n",
    "RUN pip install cloudml-hypertune\n",
    "RUN pip install tensorflow_datasets==1.3.0\n",
    "\n",
    "# Installs google cloud sdk, this is mostly for using gsutil to export model.\n",
    "RUN wget -nv \\\n",
    "    https://dl.google.com/dl/cloudsdk/release/google-cloud-sdk.tar.gz && \\\n",
    "    mkdir /root/tools && \\\n",
    "    tar xvzf google-cloud-sdk.tar.gz -C /root/tools && \\\n",
    "    rm google-cloud-sdk.tar.gz && \\\n",
    "    /root/tools/google-cloud-sdk/install.sh --usage-reporting=false \\\n",
    "        --path-update=false --bash-completion=false \\\n",
    "        --disable-installation-options && \\\n",
    "    rm -rf /root/.config/* && \\\n",
    "    ln -s /root/.config /config && \\\n",
    "    # Remove the backup directory that gcloud creates\n",
    "    rm -rf /root/tools/google-cloud-sdk/.install/.backup\n",
    "\n",
    "# Path configuration\n",
    "ENV PATH $PATH:/root/tools/google-cloud-sdk/bin\n",
    "# Make sure gsutil will use the default service account\n",
    "RUN echo '[GoogleCompute]\\nservice_account = default' > /etc/boto.cfg\n",
    "\n",
    "run mkdir /root/trainer\n",
    "COPY trainer/task.py /root/trainer/task.py\n",
    "\n",
    "ENTRYPOINT [\"python\",\"-m\",\"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245d09f-5210-434a-bc77-bd9c4a37bb82",
   "metadata": {},
   "source": [
    "## Write Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57977304-ccfd-41e2-b4d8-fcfd22b97765",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "# Single Instance Training for Iris\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import hypertune\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import argparse\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_dir', dest='model_dir',\n",
    "        default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "    parser.add_argument(\n",
    "        '--max_depth',\n",
    "        help='Max depth of XGB tree',\n",
    "        default=3)\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        help='Learning rate of XGB model',\n",
    "        default=0.1)\n",
    "    parser.add_argument(\n",
    "        '--num_boost_round',\n",
    "        help='Number of boosting iterations.',\n",
    "        type=int,\n",
    "        default=10)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "    \n",
    "def main():\n",
    "    args = get_args()\n",
    "    # Download data\n",
    "    iris_data_filename = 'iris_data.csv'\n",
    "    iris_target_filename = 'iris_target.csv'\n",
    "    data_dir = 'gs://cloud-samples-data/ai-platform/iris'\n",
    "\n",
    "    # gsutil outputs everything to stderr so we need to divert it to stdout.\n",
    "    subprocess.check_call(['gsutil', 'cp', os.path.join(data_dir,\n",
    "                                                        iris_data_filename),\n",
    "                           iris_data_filename], stderr=sys.stdout)\n",
    "    subprocess.check_call(['gsutil', 'cp', os.path.join(data_dir,\n",
    "                                                        iris_target_filename),\n",
    "                           iris_target_filename], stderr=sys.stdout)\n",
    "\n",
    "\n",
    "    # Load data into pandas, then use `.values` to get NumPy arrays\n",
    "    iris_data = pd.read_csv(iris_data_filename).values\n",
    "    iris_target = pd.read_csv(iris_target_filename).values\n",
    "\n",
    "    # Convert one-column 2D array into 1D array for use with XGBoost\n",
    "    iris_target = iris_target.reshape((iris_target.size,))\n",
    "\n",
    "    # Load data into DMatrix object\n",
    "    dtrain = xgb.DMatrix(iris_data, label=iris_target)\n",
    "    \n",
    "    param = {\n",
    "        'max_depth': args.max_depth,  # the maximum depth of each tree\n",
    "        'learning_rate' : args.learning_rate,\n",
    "        'eval_metric' : 'mlogloss',\n",
    "        'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "        'num_class': 3}  # the number of classes that exist in this datset\n",
    "\n",
    "    # Train XGBoost model\n",
    "    bst = xgb.train(param, dtrain, num_boost_round=args.num_boost_round)\n",
    "    \n",
    "    # Export the classifier to a file\n",
    "    model_filename = 'model.bst'\n",
    "    bst.save_model(model_filename)\n",
    "\n",
    "    # Upload the saved model file to Cloud Storage\n",
    "    gcs_model_path = os.path.join(args.model_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path],\n",
    "        stderr=sys.stdout)\n",
    "\n",
    "    # Start hp tuning metrics\n",
    "    # Using dtrain for sample purposes. \n",
    "    mlogloss = float(bst.eval(dtrain).split(':')[1])\n",
    "    # Report the mlogloss as hyperparameter tuning objective metric.\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='mlogloss',\n",
    "        metric_value=mlogloss,\n",
    "        global_step=1)\n",
    "\n",
    "    gcs_model_path = os.path.join(args.model_dir, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path],\n",
    "        stderr=sys.stdout)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8709fa4d",
   "metadata": {},
   "source": [
    "## Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476db69b-04aa-4404-9c14-1aa569159927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_URI=f\"gcr.io/{PROJECT_ID}/iris:hypertune\"\n",
    "!docker build -f Dockerfile -t $IMAGE_URI ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d28ff-0943-497b-88ad-fecdc151389e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run -e AIP_MODEL_DIR=\"/\" $IMAGE_URI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320d05f-8b42-4727-a581-3e562fd0ad84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa2721",
   "metadata": {},
   "source": [
    "## Deploy HPT Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484f804-91ce-496e-863c-2939a151e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\"\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "metric_spec={\"mlogloss\" : \"minimize\"}\n",
    "parameter_spec = {\n",
    "    \"learning_rate\" : hpt.DoubleParameterSpec(min=0.01, max=0.9, scale=\"log\"),\n",
    "    \"max_depth\" : hpt.IntegerParameterSpec(min=3,max=10,scale=\"linear\"),\n",
    "    \"num_boost_round\" : hpt.IntegerParameterSpec(min=1,max=30,scale=\"linear\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd6cce-e900-42e8-b662-07635200aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aip.CustomJob(\n",
    "    display_name=\"iris_\" + TIMESTAMP,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016503b-5649-4d77-9523-c94fac9385f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hp_job = aip.HyperparameterTuningJob(\n",
    "    display_name=\"iris_\" + TIMESTAMP,\n",
    "    custom_job=job,\n",
    "    metric_spec=metric_spec,\n",
    "    parameter_spec=parameter_spec,\n",
    "    max_trial_count=15,\n",
    "    parallel_trial_count=5\n",
    ")\n",
    "hp_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1f391-f817-482f-ac07-2ff5766dd7cb",
   "metadata": {},
   "source": [
    "## Train a model with conditional parameters\n",
    "### Create and run custom training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe69c2-66d6-4b42-8096-71117991eb47",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\"\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"python_package_spec\" : {\n",
    "            \"executor_image_uri\" : TRAIN_IMAGE,\n",
    "            \"package_uris\" : [f\"${BUCKET_NAME}/trainer_iris.tar.gz\"],\n",
    "            \"python_module\" : \"custom/trainer/task.py\"\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(worker_pool_specs)\n",
    "\n",
    "metric = {\n",
    "    \"metric_id\" : \"mlogloss\",\n",
    "    \"goal\" : aip.gapic.StudySpec.MetricSpec.GoalType.MINIMIZE,\n",
    "}\n",
    "conditional_parameter_learning_rate = {\n",
    "    \"parameter_spec\": {\n",
    "        \"parameter_id\": \"learning_rate\",\n",
    "        \"double_value_spec\": {\"min_value\": 1e-07, \"max_value\": 1},\n",
    "        \"scale_type\": aip.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    },\n",
    "    \"parent_discrete_values\": {\"values\": [4, 8, 16]},\n",
    "}\n",
    "conditional_parameter_num_boost_round = {\n",
    "    \"parameter_spec\": {\n",
    "        \"parameter_id\": \"num_boost_round\",\n",
    "        \"discrete_value_spec\": {\"values\": [4, 8, 16, 32, 64, 128]},\n",
    "        \"scale_type\": aip.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    },\n",
    "    \"parent_discrete_values\": {\"values\": [32, 64]},\n",
    "}\n",
    "parameter = {\n",
    "    \"parameter_id\": \"max_depth\",\n",
    "    \"discrete_value_spec\": {\"values\": [4, 8, 16, 32, 64, 128]},\n",
    "    \"scale_type\": aip.gapic.StudySpec.ParameterSpec.ScaleType.UNIT_LINEAR_SCALE,\n",
    "    \"conditional_parameter_specs\": [\n",
    "        conditional_parameter_learning_rate,\n",
    "        conditional_parameter_num_boost_round\n",
    "    ],\n",
    "}\n",
    "hyperparameter_tuning_job = {\n",
    "    \"display_name\": \"hpt\",\n",
    "    \"max_trial_count\": 4,\n",
    "    \"parallel_trial_count\": 2,\n",
    "    \"study_spec\": {\n",
    "        \"metrics\": [metric],\n",
    "        \"parameters\": [parameter],\n",
    "        \"algorithm\": aip.gapic.StudySpec.Algorithm.RANDOM_SEARCH,\n",
    "    },\n",
    "    \"trial_job_spec\": {\"worker_pool_specs\": worker_pool_specs},\n",
    "}\n",
    "\n",
    "client_options = {\"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"}\n",
    "client = aip.gapic.JobServiceClient(client_options=client_options)\n",
    "\n",
    "response = client.create_hyperparameter_tuning_job(\n",
    "        parent=f\"projects/{PROJECT_ID}/locations/us-central1\", hyperparameter_tuning_job=hyperparameter_tuning_job\n",
    "    )\n",
    "print(\"response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc9d8c-d552-4aaa-a1c3-8ae635ccf0bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Other Resources\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/migration/UJ9%20Vertex%20SDK%20Custom%20XGBoost%20with%20pre-built%20training%20container.ipynb\n",
    "* https://github.com/GoogleCloudPlatform/cloudml-samples/tree/wenzhel-sklearn/xgboost/iris\n",
    "* https://sararobinson.dev/2019/09/12/hyperparameter-tuning-xgboost.html\n",
    "* https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#aiplatform_create_hyperparameter_tuning_job_python_package_sample-python"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
