{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0404d51f-95d5-480f-bf8c-4b8323cd38ba",
   "metadata": {},
   "source": [
    "## Simple TFX Pipeline Tutorial using Penguin dataset\n",
    "\n",
    "\n",
    "In this notebook-based tutorial, we will create and run a TFX pipeline for a simple classification model. The pipeline will consist of three essential TFX components: ExampleGen, Trainer and Pusher. The pipeline includes the most minimal ML workflow like importing data, training a model and exporting the trained model.\n",
    "\n",
    "Please see Understanding TFX Pipelines to learn more about various concepts in TFX.\n",
    "\n",
    "### Install TFX"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60221fce-c8dd-42b3-a960-15ea188ea30b",
   "metadata": {},
   "source": [
    "pip install -U tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69439f88-17b0-4b7a-9595-f6feb43f378f",
   "metadata": {},
   "source": [
    "Check the TensorFlow and TFX versions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ca9362-145c-48c1-9c00-00ae5077fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "TFX version: 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05922e0d-3f2b-4150-aaa2-3b297cdcb5e5",
   "metadata": {},
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d3e086-3a03-4044-a550-b4801a7591cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_NAME = \"penguin-simple\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9710ef-1558-491f-a3cd-e6b52d84d436",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We will download the example dataset for use in our TFX pipeline. The dataset we are using is [Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/articles/intro.html) dataset which is also used in other TFX examples.\n",
    "\n",
    "There are four numeric features in this dataset:\n",
    "\n",
    "- culmen_length_mm\n",
    "- culmen_depth_mm\n",
    "- flipper_length_mm\n",
    "- body_mass_g\n",
    "\n",
    "All features were already normalized to have range [0,1]. We will build a classification model which predicts the species of penguins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b9b66-b8ea-4e7e-8f76-00812595d9fe",
   "metadata": {},
   "source": [
    "#### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f85a61ea-14b6-4f4a-811b-5368c2ecd370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/tmp/tfx-databvfhnc17/data.csv', <http.client.HTTPMessage at 0x7f1e0b614410>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tempfile\n",
    "\n",
    "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
    "_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n",
    "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
    "urllib.request.urlretrieve(_data_url, _data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b9e580-1cb2-4982-944e-e68eb13c59cf",
   "metadata": {},
   "source": [
    "#### Review the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "432dcf86-e86d-421f-ac65-9e2ae7a1ae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/tfx-data39qyy778/data.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 25.0 KiB/ 25.0 KiB]                                                \n",
      "Operation completed over 1 objects/25.0 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp '/tmp/tfx-data39qyy778/data.csv' gs://demogct-levi/dataset/penguin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eea2a77-7301-445a-a676-aa567a1f3e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://demogct-levi/dataset/penguin/data.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://demogct-levi/dataset/penguin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de11ed8a-38f2-409f-93ed-f7f5831a9bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.269091</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.298182</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.152778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.261818</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.263889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  culmen_length_mm  culmen_depth_mm  flipper_length_mm  body_mass_g\n",
       "0        0          0.254545         0.666667           0.152542     0.291667\n",
       "1        0          0.269091         0.511905           0.237288     0.305556\n",
       "2        0          0.298182         0.583333           0.389831     0.152778\n",
       "3        0          0.167273         0.738095           0.355932     0.208333\n",
       "4        0          0.261818         0.892857           0.305085     0.263889"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('gs://demogct-levi/dataset/penguin/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad30ac1d-e1ac-425e-8d5c-c0fa3f96d875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>334.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>334.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.922156</td>\n",
       "      <td>0.432520</td>\n",
       "      <td>0.483390</td>\n",
       "      <td>0.491779</td>\n",
       "      <td>0.419182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.890349</td>\n",
       "      <td>0.198564</td>\n",
       "      <td>0.234275</td>\n",
       "      <td>0.237664</td>\n",
       "      <td>0.223566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269091</td>\n",
       "      <td>0.297619</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.236111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.450909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.599091</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.581597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          species  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "count  334.000000        334.000000       334.000000         334.000000   \n",
       "mean     0.922156          0.432520         0.483390           0.491779   \n",
       "std      0.890349          0.198564         0.234275           0.237664   \n",
       "min      0.000000          0.000000         0.000000           0.000000   \n",
       "25%      0.000000          0.269091         0.297619           0.305085   \n",
       "50%      1.000000          0.450909         0.500000           0.423729   \n",
       "75%      2.000000          0.599091         0.666667           0.694915   \n",
       "max      2.000000          1.000000         1.000000           1.000000   \n",
       "\n",
       "       body_mass_g  \n",
       "count   334.000000  \n",
       "mean      0.419182  \n",
       "std       0.223566  \n",
       "min       0.000000  \n",
       "25%       0.236111  \n",
       "50%       0.375000  \n",
       "75%       0.581597  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a477b-421a-4394-9a5a-2470224420c2",
   "metadata": {},
   "source": [
    "### Create a pipeline\n",
    "TFX pipelines are defined using Python APIs. We will define a pipeline which consists of following three components.\n",
    "\n",
    "**CsvExampleGen**: Reads in data files and convert them to TFX internal format for further processing. There are multiple ExampleGens for various formats. In this tutorial, we will use CsvExampleGen which takes CSV file input.\n",
    "\n",
    "**Trainer**: Trains an ML model. Trainer component requires a model definition code from users. You can use TensorFlow APIs to specify how to train a model and save it in a _savedmodel format.\n",
    "\n",
    "**Pusher**: Copies the trained model outside of the TFX pipeline. Pusher component can be thought of an deployment process of the trained ML model.\n",
    "Before actually define the pipeline, we need to write a model code for the Trainer component first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e57052f-10cb-4898-ab0e-a43a5d3cae38",
   "metadata": {},
   "source": [
    "#### Write model training code\n",
    "We will create a simple DNN model for classification using TensorFlow Keras API. This model training code will be saved to a separate file.\n",
    "\n",
    "In this tutorial we will use Generic Trainer of TFX which support Keras-based models. You need to write a Python file containing **run_fn** function, which is the entrypoint for the Trainer component.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c43df7d-770e-4d9b-b85d-31ea6da975fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file = 'penguin_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91477e9e-6e4e-41bf-9758-264dbf6e5df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting penguin_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_FEATURE_KEYS = [\n",
    "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
    "]\n",
    "_LABEL_KEY = 'species'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "# Since we're not generating or creating a schema, we will instead create\n",
    "# a feature spec.  Since there are a fairly small number of features this is\n",
    "# manageable for this dataset.\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "           for feature in _FEATURE_KEYS\n",
    "       },\n",
    "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    schema: schema of the input data.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-2),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
    "  # version provided by pipeline author. A schema can also derived from TFT\n",
    "  # graph if a Transform component is used. In the case when either is missing,\n",
    "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
    "  # feature_spec, but the schema returned would be very primitive.\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "  # directory.\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141269cc-36bd-4a2f-80bb-7fe0fa8762a0",
   "metadata": {},
   "source": [
    "Now you have completed all preparation steps to build a TFX pipeline.\n",
    "\n",
    "### Write a pipeline definition\n",
    "We define a function to create a TFX pipeline. A Pipeline object represents a TFX pipeline which can be run using one of pipeline orchestration systems that TFX supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c01bd52f-0fb7-4044-8bbb-0ca0f5bebecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
    "\n",
    "  # Pushes the model to a filesystem destination.\n",
    "  pusher = tfx.components.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      push_destination=tfx.proto.PushDestination(\n",
    "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir)))\n",
    "\n",
    "  # Following three components will be included in the pipeline.\n",
    "  components = [\n",
    "      example_gen,\n",
    "      trainer,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1902b-aa9e-4963-9884-2e170b6f54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "_beam_pipeline_args = [\n",
    "    '--direct_running_mode=multi_processing',\n",
    "    '--direct_num_workers=0',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93c27e92-acee-42b3-b149-1b2fc38aa1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://demogct-levi/pipeline_root/penguin-vertex-training\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'demogct'     # <--- ENTER THIS\n",
    "GOOGLE_CLOUD_REGION = 'us-central1'      # <--- ENTER THIS\n",
    "GCS_BUCKET_NAME = 'demogct-levi'          # <--- ENTER THIS\n",
    "PIPELINE_NAME = 'penguin-vertex-training'\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' data.\n",
    "DATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Name of Vertex AI Endpoint.\n",
    "ENDPOINT_NAME = 'prediction-' + PIPELINE_NAME\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f08b4cc4-6604-453a-a6f2-f1daa8dd9e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://demogct-levi/data/penguin-vertex-training'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bc04d9d-1361-4fcd-949e-0bf163093a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 25.0 KiB/ 25.0 KiB]                                                \n",
      "Operation completed over 1 objects/25.0 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://download.tensorflow.org/data/palmer_penguins/penguins_processed.csv {DATA_ROOT}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd109697-c856-48f9-8048-b4db4a238dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\n",
      "0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\n",
      "0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\n",
      "0,0.29818181818181805,0.5833333333333334,0.3898305084745763,0.1527777777777778\n",
      "0,0.16727272727272732,0.7380952380952381,0.3559322033898305,0.20833333333333334\n",
      "0,0.26181818181818167,0.892857142857143,0.3050847457627119,0.2638888888888889\n",
      "0,0.24727272727272717,0.5595238095238096,0.15254237288135594,0.2569444444444444\n",
      "0,0.25818181818181823,0.773809523809524,0.3898305084745763,0.5486111111111112\n",
      "0,0.32727272727272727,0.5357142857142859,0.1694915254237288,0.1388888888888889\n",
      "0,0.23636363636363636,0.9642857142857142,0.3220338983050847,0.3055555555555556\n"
     ]
    }
   ],
   "source": [
    "!gsutil cat {DATA_ROOT}/penguins_processed.csv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1755c957-90cd-4d83-b9f2-67bce61fac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://penguin_trainer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  3.7 KiB/  3.7 KiB]                                                \n",
      "Operation completed over 1 objects/3.7 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {_trainer_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fb8b4b3-fb2b-40f9-ad20-a639b2f6d677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://demogct-levi/pipeline_module/penguin-vertex-training'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODULE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3d74205-4fed-4d08-8c43-18d0706ce081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://demogct-levi/data/penguin-vertex-training/penguins_processed.csv\n"
     ]
    }
   ],
   "source": [
    "!echo {DATA_ROOT}/penguins_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0662c37f-b2f8-46ed-87bd-6c4d5e45facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root=\"gs://demogct-levi/data/penguin-vertex-training/penguins_processed.csv\"\n",
    "example_gen = tfx.components.CsvExampleGen(input_base=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fdda707-2834-4219-8f58-cf8cbee3e0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CsvExampleGen(spec: <tfx.types.standard_component_specs.FileBasedExampleGenSpec object at 0x7f1e0b62df10>, executor_spec: <tfx.dsl.components.base.executor_spec.BeamExecutorSpec object at 0x7f1e0b62dbd0>, driver_class: <class 'tfx.components.example_gen.driver.FileBasedDriver'>, component_id: CsvExampleGen, inputs: {}, outputs: {'examples': OutputChannel(artifact_type=Examples, producer_component_id=CsvExampleGen, output_key=examples, additional_properties={}, additional_custom_properties={})})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exaample_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6642411b-5ec7-4770-93bb-0d3946f0cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, endpoint_name: str, project_id: str,\n",
    "                     region: str, use_gpu: bool) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Implements the penguin pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline or otherwise joins/converts training data.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # NEW: Configuration for Vertex AI Training.\n",
    "  # This dictionary will be passed as `CustomJobSpec`.\n",
    "  vertex_job_spec = {\n",
    "      'project': project_id,\n",
    "      'worker_pool_specs': [{\n",
    "          'machine_spec': {\n",
    "              'machine_type': 'n1-standard-4',\n",
    "          },\n",
    "          'replica_count': 1,\n",
    "          'container_spec': {\n",
    "              'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
    "          },\n",
    "      }],\n",
    "  }\n",
    "  if use_gpu:\n",
    "    # See https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#acceleratortype\n",
    "    # for available machine types.\n",
    "    vertex_job_spec['worker_pool_specs'][0]['machine_spec'].update({\n",
    "        'accelerator_type': 'NVIDIA_TESLA_K80',\n",
    "        'accelerator_count': 1\n",
    "    })\n",
    "\n",
    "  # Trains a model using Vertex AI Training.\n",
    "  # NEW: We need to specify a Trainer for GCP with related configs.\n",
    "  trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=5),\n",
    "      custom_config={\n",
    "          tfx.extensions.google_cloud_ai_platform.ENABLE_UCAIP_KEY:\n",
    "              True,\n",
    "          tfx.extensions.google_cloud_ai_platform.UCAIP_REGION_KEY:\n",
    "              region,\n",
    "          tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
    "              vertex_job_spec,\n",
    "          'use_gpu':\n",
    "              use_gpu,\n",
    "      })\n",
    "\n",
    "  # NEW: Configuration for pusher.\n",
    "  vertex_serving_spec = {\n",
    "      'project_id': project_id,\n",
    "      'endpoint_name': endpoint_name,\n",
    "      # Remaining argument is passed to aiplatform.Model.deploy()\n",
    "      # See https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api#deploy_the_model\n",
    "      # for the detail.\n",
    "      #\n",
    "      # Machine type is the compute resource to serve prediction requests.\n",
    "      # See https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types\n",
    "      # for available machine types and acccerators.\n",
    "      'machine_type': 'n1-standard-4',\n",
    "  }\n",
    "\n",
    "  # Vertex AI provides pre-built containers with various configurations for\n",
    "  # serving.\n",
    "  # See https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "  # for available container images.\n",
    "  serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n",
    "  if use_gpu:\n",
    "    vertex_serving_spec.update({\n",
    "        'accelerator_type': 'NVIDIA_TESLA_K80',\n",
    "        'accelerator_count': 1\n",
    "    })\n",
    "    serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-6:latest'\n",
    "\n",
    "  # NEW: Pushes the model to Vertex AI.\n",
    "  pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      custom_config={\n",
    "          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n",
    "              serving_image,\n",
    "          tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n",
    "            vertex_serving_spec,\n",
    "      })\n",
    "\n",
    "  components = [\n",
    "      example_gen,\n",
    "      trainer,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cddd576-f9cd-40ef-bef5-f192d855310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
    "\n",
    "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        project_id=GOOGLE_CLOUD_PROJECT,\n",
    "        region=GOOGLE_CLOUD_REGION,\n",
    "        # We will use CPUs only for now.\n",
    "        use_gpu=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8287b63-9bb0-476d-8879-90e8ac1ef607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/313681173937/locations/us-central1/pipelineJobs/penguin-vertex-training-20220318175655\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/313681173937/locations/us-central1/pipelineJobs/penguin-vertex-training-20220318175655')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/penguin-vertex-training-20220318175655?project=313681173937\n"
     ]
    }
   ],
   "source": [
    "# docs_infra: no_execute\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME)\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352391e7-76ba-47ae-8ace-b50d7c528b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
