{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1140f0-b72f-4140-b4d1-e3f7e1b79f3f",
   "metadata": {},
   "source": [
    "## Using Vertex ML Metadata with Pipelines\n",
    "\n",
    "In this lab, you will learn how to analyze metadata from your Vertex Pipelines runs with Vertex ML Metadata.\n",
    "\n",
    "What you learn\n",
    "You'll learn how to:\n",
    "\n",
    "- Use the Kubeflow Pipelines SDK to build an ML pipeline that creates a dataset in Vertex AI, and trains and deploys a custom Scikit-learn model on that dataset\n",
    "- Write custom pipeline components that generate artifacts and metadata\n",
    "- Compare Vertex Pipelines runs, both in the Cloud console and programmatically\n",
    "- Trace the lineage for pipeline-generated artifacts\n",
    "- Query your pipeline run metadata\n",
    "\n",
    "\n",
    "\n",
    "The focus of this lab is on understanding metadata from pipeline runs. In order to do that, we'll need a pipeline to run on Vertex Pipelines, which is where we'll start. Here we'll define a 3-step pipeline with the following custom components:\n",
    "\n",
    "get_dataframe: Retrieve data from a BigQuery table and convert it into a Pandas DataFrame\n",
    "train_sklearn_model: Use the Pandas DataFrame to train and export a Scikit Learn model, along with some metrics\n",
    "deploy_model: Deploy the exported Scikit Learn model to an endpoint in Vertex AI\n",
    "In this pipeline, we'll use the UCI Machine Learning Dry beans dataset, from: KOKLU, M. and OZKAN, I.A., (2020), \"Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.\"¬ùIn Computers and Electronics in Agriculture, 174, 105507. DOI.\n",
    "\n",
    "This is a tabular dataset, and in our pipeline we'll use the dataset to train, evaluate, and deploy a Scikit-learn model that classifies beans into one of 7 types based on their characteristics. Let's start coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4debccfc-c620-4597-b700-b1d84bdd2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# We'll use this namespace for metadata querying\n",
    "from google.cloud import aiplatform_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be222b7-8250-48e0-a675-f68a014f8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"gs://demogct-wd\"\n",
    "REGION=\"us-central1\"\n",
    "PROJECT_ID=\"demogct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d46f9fc-86d1-45f7-9760-9f30feb0980e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://demogct-wd/pipeline_root/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3b43c-abb1-448f-a41c-54986d1b7f16",
   "metadata": {},
   "source": [
    "### Create Dataframe componenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed169638-aca5-4623-abb0-bb33b9f91aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery\", \"pandas\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"create_dataset.yaml\"\n",
    ")\n",
    "def get_dataframe(\n",
    "    bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\")\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "\n",
    "    bqclient = bigquery.Client()\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_table\n",
    "    )\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "    )\n",
    "    dataframe = rows.to_dataframe(\n",
    "        create_bqstorage_client=True,\n",
    "    )\n",
    "    dataframe = dataframe.sample(frac=1, random_state=2)\n",
    "    dataframe.to_csv(output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d27260-24a8-4fa5-8bc7-ff4de34de157",
   "metadata": {},
   "source": [
    "Let's take a closer look at what's happening in this component:\n",
    "\n",
    "- The @component decorator compiles this function to a component when the pipeline is run. You'll use this anytime you write a custom component.\n",
    "- The base_image parameter specifies the container image this component will use.\n",
    "\n",
    "- This component will use a few Python libraries, which we specify via the *packages_to_install* parameter.\n",
    "\n",
    "- The output_component_file parameter is optional, and specifies the yaml file to write the compiled component to. After running the cell you should see that file written to your notebook instance. \n",
    "- Next, this component uses the BigQuery Python client library to download our data from BigQuery into a Pandas DataFrame, and then creates an output artifact of that data as a CSV file. This will be passed as input to our next component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66600f90-9831-4c89-b0bc-626f2350c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(\n",
    "    packages_to_install=[\"sklearn\", \"pandas\", \"joblib\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"beans_model_component.yaml\",\n",
    ")\n",
    "def sklearn_train(\n",
    "    dataset: Input[Dataset],\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model]\n",
    "):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from joblib import dump\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(dataset.path)\n",
    "    labels = df.pop(\"Class\").tolist()\n",
    "    data = df.values.tolist()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "\n",
    "    skmodel = DecisionTreeClassifier()\n",
    "    skmodel.fit(x_train,y_train)\n",
    "    score = skmodel.score(x_test,y_test)\n",
    "    print('accuracy is:',score)\n",
    "\n",
    "    metrics.log_metric(\"accuracy\",(score * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"Scikit Learn\")\n",
    "    metrics.log_metric(\"dataset_size\", len(df))\n",
    "    dump(skmodel, model.path + \".joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136d029-2325-458d-b645-0cddcd94bf29",
   "metadata": {},
   "source": [
    "### Define a component to upload and deploy the model to Vertex AI\n",
    "\n",
    "Finally, our last component will take the trained model from the previous step, upload it to Vertex AI, and deploy it to an endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21071990-5b81-4b3c-9e2e-fcb3d4d7d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=\"beans_deploy_component.yaml\",\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"beans-model-pipeline\",\n",
    "        artifact_uri = model.uri.replace(\"model\", \"\"),\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f2296-e88e-4b44-b5a6-520f39071198",
   "metadata": {},
   "source": [
    "### Define and compile the component\n",
    "\n",
    "Now that we've defined our three components, next we'll create our pipeline definition. This describes how input and output artifacts flow between steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bc7586c-ea36-4b4b-ba72-20c83fb2508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "\n",
    "@pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline.\n",
    "    name=\"mlmd-pipeline\"\n",
    ")\n",
    "def pipeline(\n",
    "    bq_table: str = \"\",\n",
    "    output_data_path: str = \"data.csv\",\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION\n",
    "):\n",
    "    #BQ Read data from the components\n",
    "    dataset_task = get_dataframe(bq_table)\n",
    "\n",
    "    model_task = sklearn_train(\n",
    "        dataset_task.output\n",
    "    )\n",
    "\n",
    "    deploy_task = deploy_model(\n",
    "        model=model_task.outputs[\"model\"],\n",
    "        project=project,\n",
    "        region=region\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb8075f8-d948-488d-a0e5-74a7bdcee76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1266: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"mlmd_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271739c3-0984-4af5-865d-d65f704664d6",
   "metadata": {},
   "source": [
    "### Start two pipeline runs\n",
    "Next we'll kick off two runs of our pipeline. First let's define a timestamp to use for our pipeline job IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13ca0714-ea90-432a-8fc6-11d1e83d316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7014b4f-e962-4b43-b6d4-8505e70d9024",
   "metadata": {},
   "source": [
    "Remember that our pipeline takes one parameter when we run it: the bq_table we want to use for training data. This pipeline run will use a smaller version of the beans dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67edc6a5-4b44-4598-86a0-ed6df47574a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the small Data Set Run\n",
    "run1 = aiplatform.PipelineJob(\n",
    "    display_name=\"mlmd-pipeline\",\n",
    "    template_path=\"mlmd_pipeline.json\",\n",
    "    job_id=\"mlmd-pipeline-small-{0}\".format(TIMESTAMP),\n",
    "    parameter_values={\"bq_table\": \"sara-vertex-demos.beans_demo.small_dataset\"},\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f89d9c-ec48-44d3-9804-03f1e77b74dd",
   "metadata": {},
   "source": [
    "The next snippet is with larger dataset run\n",
    "- parameter_values={\"bq_table\": \"sara-vertex-demos.beans_demo.**large_dataset**\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba58cdce-a33e-498b-be9a-8d190aa6611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run2 = aiplatform.PipelineJob(\n",
    "    display_name=\"mlmd-pipeline\",\n",
    "    template_path=\"mlmd_pipeline.json\",\n",
    "    job_id=\"mlmd-pipeline-large-{0}\".format(TIMESTAMP),\n",
    "    parameter_values={\"bq_table\": \"metr\"},\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f16dd2-a8cd-4c48-ae77-5e41a7234526",
   "metadata": {},
   "source": [
    "Finally, kick off pipeline executions for both runs. It's best to do this in two separate notebook cells so you can see the output for each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5cbb5eb-3b02-4f8d-8a2b-78f23373340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/313681173937/locations/us-central1/pipelineJobs/mlmd-pipeline-small-20211115013533\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/313681173937/locations/us-central1/pipelineJobs/mlmd-pipeline-small-20211115013533')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlmd-pipeline-small-20211115013533?project=313681173937\n"
     ]
    }
   ],
   "source": [
    "run1.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06dfa6f-0dd5-4919-a04c-c454828154dc",
   "metadata": {},
   "source": [
    "#### Submit the second Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd671b1e-3282-446a-9a39-71cfd5f815bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/313681173937/locations/us-central1/pipelineJobs/mlmd-pipeline-large-20211115013533\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/313681173937/locations/us-central1/pipelineJobs/mlmd-pipeline-large-20211115013533')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mlmd-pipeline-large-20211115013533?project=313681173937\n"
     ]
    }
   ],
   "source": [
    "run2.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca9ef3-bc16-48c3-b9c9-8a2650b1389b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
