{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab60d19-91a2-4451-a546-b2ed28dd3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner \n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a85e66-5b6f-49b9-857c-2cb80d3ee3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables\n",
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "MODEL_PATH='gs://'+PROJECT_ID+'-bucket/model/'\n",
    "DATASET_PATH='gs://'+PROJECT_ID+'/area_cover_dataset.csv'\n",
    "PIPELINE_ROOT = 'gs://'+PROJECT_ID\n",
    "MODEL_ARTIFACTS_LOCATION ='gs://'+PROJECT_ID+'-bucket/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42396aa8-ca07-4aa2-9301-0ac8e8e700e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the area_cover_dataset csv data into pandas dataframe\n",
    "area_cover_dataframe = pandas.read_csv(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07eb6c2-d7e1-4075-aff1-f2c6582a8bc7",
   "metadata": {},
   "source": [
    "**Task 4** Create the function that converts categorical data to indexed integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ba233-6650-4b05-b832-d846609f86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes the area cover dataframe and converts the two categorical (string) columns into indexed values\n",
    "def index(dataframe):\n",
    "    \n",
    "    [ TODO - Insert your Code ]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd67b4-142e-4772-9514-97c4ce735a0b",
   "metadata": {},
   "source": [
    "**Task 5** Extract the feature columns and standardize the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4df68-a401-4274-bead-d32ef844377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature columns into a new dataframe called scaler_features that has been standardized using the sklearn.preprocessing.StandardScaler method.\n",
    "# The features are all columns from the area cover dataset except the \"Area_Cover\" column\n",
    "indexed_dataframe = index(area_cover_dataframe)\n",
    "features_dataframe = indexed_dataframe.drop(\"Area_Cover\", axis = 1)\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "scaled_features = [ TODO - Insert your code ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cccc50-4302-4c5f-9deb-961993a64d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary matrix containing the categorical Area_Cover column data converted using keras.utils.to_categorical()\n",
    "labels_dataframe = indexed_dataframe[\"Area_Cover\"]\n",
    "categorical_labels = to_categorical(labels_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e7306-3980-45c5-8cf9-e25879714261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into model training and validation data\n",
    "dfx_train, dfx_val, dfy_train, dfy_val = train_test_split(scaled_features.values, categorical_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e9fab-52e5-47cd-a30a-1ef367cdaff8",
   "metadata": {},
   "source": [
    "**Task 6** Create a function that returns a sequential categorical model function with a hyperparameter tuning layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501648f6-defd-48c0-8ea4-e7133a85cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that returns a sequential categorical model function with a hyperparameter tuning layer\n",
    "def build_model(hptune):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape = (12,), activation = \"relu\"))\n",
    "    \n",
    "    [ TODO - Insert your code ]\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e96334-1c5d-4a3e-8163-5dc7aa30025b",
   "metadata": {},
   "source": [
    "**Task 7** Create a Keras Hyperband Hyperparameter tuner with an accuracy objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3d1d0-7b7a-4b92-8c85-e8e33ebcb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras Hyperband Hyperparameter tuner with an accuracy objective\n",
    "\n",
    "tuner =  [ TODO - Insert your code ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4f814-939a-45fd-a232-4bbf18f6e657",
   "metadata": {},
   "source": [
    "**Task 8** Perform Hyperparameter tuning and train the optimal model\n",
    "\n",
    "You do not have to add any of your own code for this task. Run the cells to tune, optimize and train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20fa022-34e6-4002-80fc-bd9891968da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an early stopping callback using that stops when the validation loss quantity does not improve after 5 epochs\n",
    "stop_early = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Perform a Keras Tuner Search for the best hyperparameter configurations using the training data split over 50 epochs\n",
    "tuner.search(dfx_train, dfy_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters for the model as determined from the search\n",
    "best_hyperparameters=tuner.get_best_hyperparameters(num_trials=10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3872d0f-d519-4836-978f-4fd75caf5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model using the best_hyperparameters and train it. \n",
    "model = tuner.hypermodel.build(best_hyperparameters)\n",
    "history = model.fit(dfx_train, dfy_train, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98cdbd-13ce-4abc-abf6-b95e8df87f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model training history find and print out the epoch with the best validation accuracy. \n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b3543-7e34-4d89-a24c-4bed0fa0f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the Model test loss and test accuracy by evaluating the validation data split. \n",
    "eval_result = model.evaluate(dfx_val, dfy_val)\n",
    "print(\"[Model test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eb393-2586-44b8-a407-f3d5f5425bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model (hypermodel) using the best_hyperparameters and retrain. \n",
    "hypermodel = tuner.hypermodel.build(best_hyperparameters)\n",
    "# Retrain the model using the number of epochs that was previously determined to be the best. \n",
    "hypermodel.fit(dfx_train, dfy_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b5c52-354a-4b7d-a114-c05864713f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the test loss and test accuracy for hypermodel by evaluating the validation data split. \n",
    "eval_result = hypermodel.evaluate(dfx_val, dfy_val)\n",
    "print(\"[Hypermodel test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165b086-9077-473c-b6eb-75bfb8fee14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the hypertuned model\n",
    "# NB the MODEL_PATH bucket must be created before this will succeed and it must be in the same location as the model.\n",
    "# e.g. gsutil mb -l us-central1  gs://${PROJECT_ID}-bucket\n",
    "hypermodel.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48c72f-8554-4e5d-91c6-b3a3c76bf0ee",
   "metadata": {},
   "source": [
    "**Task 9** Create a Custom Container for Vertex AI pipeline model training\n",
    "1. Create a Python model trainer module using the above code\n",
    "2. Save the code as `model.py` in the `model/trainer` beneath the current working directory for this notebook\n",
    "3. Make sure you set the Project ID correctly in the Python script. \n",
    "4. Create the Dockerfile definition in the `model/` directory for your custom training container using the `gcr.io/deeplearning-platform-release/tf2-cpu.2-6` base container image\n",
    "\n",
    "Once you have prepared the custom container Python module code and Dockerfile you can build and test the custom container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752898e-d2d3-479a-b699-36b438a6cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the container using the following gcr.io tag\n",
    "IMAGE_URI=\"gcr.io/{}/tensorflow:latest\".format(PROJECT_ID)\n",
    "!docker build ~/model/. -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b3e00-773e-48c1-bca0-d53b0cd60fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the docker image locally to test it\n",
    "!docker run $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aba246-f866-49d6-b1ab-1a752ede3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the docker image to the Google container registry\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226af430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install kubeflow pipeline SDK and google cloud pipeline component for building Vertex AI pipelines\n",
    "!pip3 install kfp google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d386dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries required for Vertext AI pipelines\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f1aed-476a-47d4-bc9d-93b6dea74a4a",
   "metadata": {},
   "source": [
    "**Task 10** Define the Vertex AI Training pipeline\n",
    "\n",
    "1. Add your code for the Training Operation using your newly created custom container\n",
    "    * This should reference the custom container_uri passed in as a parameter\n",
    "    * This should use \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\" for the `model_serving_container_image_uri`\n",
    "2. Add your code for the Model Deploy Operation\n",
    "    * This operation should output a model and an endpoint.\n",
    "    \n",
    "All machine types should be specified as \"n1-standard-4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vertex AI pipeline\n",
    "@kfp.dsl.pipeline(name=\"vertex-ai-pipeline\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bucket: str = MODEL_ARTIFACTS_LOCATION,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    container_uri: str = \"\",\n",
    "):\n",
    "    \n",
    "    training_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=\"tensorflow-train-model\",\n",
    "        \n",
    "        [ TODO - Insert your code ]\n",
    "        \n",
    "    )\n",
    "       \n",
    "    create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name = \"tensorflow-model-endpoint\",\n",
    "    )\n",
    "    \n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        \n",
    "        [ TODO - Insert your code ]        \n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the  Vertex AI pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ffbb0-4b0d-40a9-bab2-85822d2d9638",
   "metadata": {},
   "source": [
    "**Task 11** Create the Vertex AI Pipeline job object\n",
    "\n",
    "The pipeline job must specified using the compiled pipeline definition JSON file and should point to your saved model location and your custom training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35996508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Vertex AI Pipeline job object\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    \n",
    "    [ TODO - Insert your code ]   \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cbf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Vertex AI pipeline job\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the model created by the pipeline\n",
    "!gcloud ai models list --region=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33838709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the endpoint ID where the model has been deployed \n",
    "\n",
    "!gcloud ai endpoints list --region=$REGION\n",
    "ENDPOINT_IDS=!gcloud ai endpoints list --region=$REGION --format=\"value(name)\" 2>/dev/null\n",
    "print(\"Vertex AI Endpoint ID:\" + ENDPOINT_IDS[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca90da2-f6f2-48aa-aa05-8e1c2c24e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in the pre-prepared sample test.json \n",
    "!gsutil cp gs://sureskills-lab-dev/CEPF/vertex-ai/test.json . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44672e50-b94e-43a0-9816-f8523feda8a7",
   "metadata": {},
   "source": [
    "**Task 12** Create a function to convert the source JSON test data to an array of normalized column values\n",
    "\n",
    "The test data consists of samples with feature data that you want to use to generate area_cover type predictions using the model endpoint. \n",
    "\n",
    "You must define a functon that performs the following tasks:\n",
    "1. Read the `test.json` instance data into a dataframe\n",
    "2. Normalize the column data using the `StandardScalar.fit_transform` method\n",
    "3. Output an array of arrays containing the normalized feature column data for each test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed3eee-c931-4ba0-bcc0-f2ab5daf8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the json test data to an array of standard scaler normalized column data\n",
    "def get_instances(file_name):\n",
    "    instances = []\n",
    "\n",
    "    [ TODO - Insert your code ]   \n",
    "\n",
    "    for _ in normalize_df.values:\n",
    "        instances.append(list(_))\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca082f9-3ec6-44e1-af13-c73b24ca1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for making predictions using the endpoint\n",
    "def endpoint_predict( project: str, location: str, instances, endpoint: str):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "    endpoint = aiplatform.Endpoint(endpoint)   \n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the result by calling get_values() that convert JSON to the numpy array\n",
    "# Replace the endpoint ID with the new ENDPOINT_ID if needed\n",
    "FILE_NAME = \"test.json\"\n",
    "instances = get_instances(FILE_NAME)\n",
    "prediction_result = endpoint_predict(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    instances=instances,\n",
    "    endpoint=ENDPOINT_IDS[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save `Area_Cover` predictions with respect to the test instance features\n",
    "area_cover_predictions={}\n",
    "for index,area_cover in enumerate(prediction_result.predictions):\n",
    "    print(index,\":\",numpy.argmax(area_cover), end=' \\n')\n",
    "    area_cover_predictions[index]=str(numpy.argmax(area_cover))\n",
    "    \n",
    "f = open(\"predictions.txt\", \"w\")\n",
    "f.write(json.dumps(area_cover_predictions))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m89"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
